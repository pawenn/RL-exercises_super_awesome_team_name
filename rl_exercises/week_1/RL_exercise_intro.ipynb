{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a37d9151-dcbf-4c7f-8d0b-c697b3fdbc13",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RL Exercise Demo\n",
    "This exercise serves as a demonstration how to quickly train an RL agent on a popular environment with a RL framework.\n",
    "We will\n",
    "1. Select and instantiate environment (gym's BipedalWalker-v3).\n",
    "2. Select and setup our RL algorithm / agent (stablebaselines3 [SAC](https://spinningup.openai.com/en/latest/algorithms/sac.html)).\n",
    "3. Train the agent on the environment and visualize training progress.\n",
    "4. Evaluate our agent and observe the distribution of test performances.\n",
    "5. Record and replay the agent, before and after."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c4b5e2-e487-4b04-8c55-3938b4d82529",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Environment: Bipedal Walker\n",
    "![Bipedal Walker](bipedal_walker.gif) [credits](https://www.gymlibrary.dev/_images/bipedal_walker.gif)\n",
    "\n",
    "![Interaction Env-Agent](env_agent.png)\n",
    "\n",
    "### Env\n",
    "- locomotion\n",
    "- 4 joints\n",
    "\n",
    "### Action Space\n",
    "- motor speed for all 4 joints (hips and knees) [-1, 1]\n",
    "\n",
    "### Observation Space\n",
    "State consists of\n",
    "- hull angle speed\n",
    "- angular velocity\n",
    "- horizontal speed\n",
    "- vertical speed\n",
    "- position of joints\n",
    "- joints angular speed\n",
    "- legs contact with ground\n",
    "- 10 lidar rangefinder measurements\n",
    "\n",
    "There are no coordinates! State vector with 24 entries.\n",
    "\n",
    "### Rewards\n",
    "Moving forward gives rewards.\n",
    "Falling is punished.\n",
    "Applying motor torque costs a little.\n",
    "\n",
    "\n",
    "### Starting State\n",
    "Stands at the left in a certain position.\n",
    "\n",
    "### Episode Termination\n",
    "- walker falls\n",
    "- or reaches end of terrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22de499d-f2cd-4d95-b61b-1827650d001b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_fn = \"trained_agent.zip\"\n",
    "env_id = \"BipedalWalker-v3\"\n",
    "log_dir = \"logs/tensorboard\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a267d6a0-dcd5-42d2-83e2-a70cf4518e36",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Agent: SAC\n",
    "SAC: Soft Actor Critic\n",
    "[[paper]](https://arxiv.org/abs/1801.01290) [[blogpost]](https://spinningup.openai.com/en/latest/algorithms/sac.html)\n",
    "\n",
    "- off-policy algorithm\n",
    "- for continuous actions\n",
    "- one actor, one critic\n",
    "- special regularization to steer exploration-exploitation trade-off "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b794a7c-86f1-4675-aa8f-9e81f2ca4f4b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to logs/tensorboard/SAC_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 456      |\n",
      "|    ep_rew_mean     | -104     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 148      |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 1822     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.9    |\n",
      "|    critic_loss     | 40.8     |\n",
      "|    ent_coef        | 0.597    |\n",
      "|    ent_coef_loss   | -3.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1721     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 645      |\n",
      "|    ep_rew_mean     | -95.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 5162     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -27.4    |\n",
      "|    critic_loss     | 0.172    |\n",
      "|    ent_coef        | 0.222    |\n",
      "|    ent_coef_loss   | -9.67    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5061     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 710      |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 8524     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.6    |\n",
      "|    critic_loss     | 0.0997   |\n",
      "|    ent_coef        | 0.0819   |\n",
      "|    ent_coef_loss   | -15.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8423     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 552      |\n",
      "|    ep_rew_mean     | -96.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 176      |\n",
      "|    time_elapsed    | 49       |\n",
      "|    total_timesteps | 8825     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.5    |\n",
      "|    critic_loss     | 36.8     |\n",
      "|    ent_coef        | 0.0751   |\n",
      "|    ent_coef_loss   | -15.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8724     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 465      |\n",
      "|    ep_rew_mean     | -99.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 9305     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -25.4    |\n",
      "|    critic_loss     | 2.08     |\n",
      "|    ent_coef        | 0.0658   |\n",
      "|    ent_coef_loss   | -14.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9204     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 402      |\n",
      "|    ep_rew_mean     | -100     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 178      |\n",
      "|    time_elapsed    | 54       |\n",
      "|    total_timesteps | 9639     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -25.6    |\n",
      "|    critic_loss     | 0.458    |\n",
      "|    ent_coef        | 0.0602   |\n",
      "|    ent_coef_loss   | -13.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9538     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 352      |\n",
      "|    ep_rew_mean     | -101     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 9868     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    ent_coef        | 0.0568   |\n",
      "|    ent_coef_loss   | -13.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9767     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 319      |\n",
      "|    ep_rew_mean     | -102     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 10214    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.3    |\n",
      "|    critic_loss     | 5.6      |\n",
      "|    ent_coef        | 0.052    |\n",
      "|    ent_coef_loss   | -12.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10113    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 291      |\n",
      "|    ep_rew_mean     | -102     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 178      |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 10460    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.2    |\n",
      "|    critic_loss     | 54.8     |\n",
      "|    ent_coef        | 0.0489   |\n",
      "|    ent_coef_loss   | -13.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10359    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 267      |\n",
      "|    ep_rew_mean     | -102     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 178      |\n",
      "|    time_elapsed    | 59       |\n",
      "|    total_timesteps | 10693    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.8    |\n",
      "|    critic_loss     | 3.97     |\n",
      "|    ent_coef        | 0.0463   |\n",
      "|    ent_coef_loss   | -12      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10592    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 252      |\n",
      "|    ep_rew_mean     | -104     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 178      |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 11073    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.6    |\n",
      "|    critic_loss     | 2.26     |\n",
      "|    ent_coef        | 0.0427   |\n",
      "|    ent_coef_loss   | -9.67    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10972    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 244      |\n",
      "|    ep_rew_mean     | -105     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 179      |\n",
      "|    time_elapsed    | 65       |\n",
      "|    total_timesteps | 11729    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.2    |\n",
      "|    critic_loss     | 2.61     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -6.59    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 11628    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 232      |\n",
      "|    ep_rew_mean     | -106     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 179      |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 12066    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.1    |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | -7.89    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 11965    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 222      |\n",
      "|    ep_rew_mean     | -107     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 12421    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.4    |\n",
      "|    critic_loss     | 2.01     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | -5.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 12320    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 213      |\n",
      "|    ep_rew_mean     | -107     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 70       |\n",
      "|    total_timesteps | 12778    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.7    |\n",
      "|    critic_loss     | 2.25     |\n",
      "|    ent_coef        | 0.0305   |\n",
      "|    ent_coef_loss   | -7.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 12677    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 204      |\n",
      "|    ep_rew_mean     | -107     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 13030    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.5    |\n",
      "|    critic_loss     | 2.16     |\n",
      "|    ent_coef        | 0.0292   |\n",
      "|    ent_coef_loss   | -4.94    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 12929    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 195      |\n",
      "|    ep_rew_mean     | -108     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 73       |\n",
      "|    total_timesteps | 13285    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.4    |\n",
      "|    critic_loss     | 5.63     |\n",
      "|    ent_coef        | 0.0281   |\n",
      "|    ent_coef_loss   | -4.72    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 13184    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 190      |\n",
      "|    ep_rew_mean     | -109     |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 75       |\n",
      "|    total_timesteps | 13656    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17      |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    ent_coef        | 0.0269   |\n",
      "|    ent_coef_loss   | -2.78    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 13555    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 184      |\n",
      "|    ep_rew_mean     | -109     |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 14001    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.1    |\n",
      "|    critic_loss     | 2.26     |\n",
      "|    ent_coef        | 0.0264   |\n",
      "|    ent_coef_loss   | -2       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 13900    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "env = gym.make(env_id)\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_dir)\n",
    "model.learn(total_timesteps=500_000)\n",
    "model.save(model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c528b3-c890-4d43-9854-6af07bfcc94f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7383350",
   "metadata": {},
   "source": [
    "## Evaluate Trained and Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cea3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "\n",
    "n_eval_episodes = 10\n",
    "\n",
    "\n",
    "# Create/ load agents\n",
    "\n",
    "# Untrained agent\n",
    "env = gym.make(env_id)\n",
    "untrained_agent = SAC(\"MlpPolicy\", env, verbose=0)\n",
    "\n",
    "# Trained agents\n",
    "trained_agent = SAC.load(model_fn)\n",
    "\n",
    "agents = {\n",
    "    \"untrained\": untrained_agent,\n",
    "    \"trained\": trained_agent,\n",
    "}\n",
    "\n",
    "# Create env to evaluate\n",
    "env = DummyVecEnv([lambda: gym.make(env_id)])\n",
    "env = Monitor(gym.make(env_id))\n",
    "\n",
    "# Evaluate each agent and gather results\n",
    "results = []\n",
    "for name, agent in agents.items():\n",
    "    # Rollout n_eval_episodes and record performance\n",
    "    means, stds = evaluate_policy(\n",
    "        agent, env, n_eval_episodes=n_eval_episodes, return_episode_rewards=True\n",
    "    )\n",
    "    performance = np.mean(means)\n",
    "    results.append(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"agent\": name,\n",
    "                \"episode\": np.arange(0, n_eval_episodes),\n",
    "                \"reward\": means,\n",
    "                \"length\": stds,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "results = pd.concat(results).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a876b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as printr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = results\n",
    "printr(df)\n",
    "\n",
    "ax = sns.boxplot(data=df, x=\"agent\", y=\"reward\")\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(data=df, x=\"agent\", y=\"length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "video_folder = \"logs/videos/\"\n",
    "video_length = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Record random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "# This creates a vectorized env (here of length 1). Is useful for parallelization\n",
    "# when we have several workers that can perform individual rollouts.\n",
    "env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "\n",
    "env = VecVideoRecorder(\n",
    "    env,\n",
    "    video_folder,\n",
    "    record_video_trigger=lambda x: x == 0,\n",
    "    video_length=video_length,\n",
    "    name_prefix=f\"random-agent-{env_id}\",\n",
    ")\n",
    "\n",
    "n_steps = video_length\n",
    "\n",
    "# Reset and initialize the environment\n",
    "obs = env.reset()\n",
    "\n",
    "# For the number of frames\n",
    "for i in range(n_steps):\n",
    "    # Sample a random action from the action space\n",
    "    action = [env.action_space.sample()]\n",
    "\n",
    "    # Step the environment\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        # If the env signals the end of the episode, stop recording\n",
    "        break\n",
    "        obs = env.reset()\n",
    "# Close / cleanup env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Record trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "\n",
    "# Record the video starting at the first step\n",
    "env = VecVideoRecorder(\n",
    "    env,\n",
    "    video_folder,\n",
    "    record_video_trigger=lambda x: x == 0,\n",
    "    video_length=video_length,\n",
    "    name_prefix=f\"trained-agent-{env_id}\",\n",
    ")\n",
    "\n",
    "model = SAC.load(model_fn)\n",
    "\n",
    "n_steps = video_length\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(n_steps):\n",
    "    # Now PREDICT the action to take with the trained model ðŸ¤–\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "        obs = env.reset()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
